\documentclass{beamer}

\usepackage{beamerthemeshadow}
%\usepackage{beamercolorthemeseahorse}
\usepackage{graphicx}
\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{beamercolorthemecrane}
\usepackage{animate}

\usepackage{xmpmulti}

%\logo{\includegraphics[scale=0.2]{logo.png}}
%\logo{teste}

\title{Aprendizagem por reforço com redes neurais}
\author{Ricardo Yamamoto Abe \\ ricardoy@ime.usp.br}

\date{11 de junho de 2018}

\begin{document}

\beamertemplatetransparentcoveredmedium

\setbeamertemplate{footline}[frame number]

\frame{\titlepage}

\begin{frame}[shrink]
\tableofcontents
\end{frame}


\section{Introdução}

\subsection{Objetivo}

\frame{
  \frametitle{Objetivo}
  
  \begin{block}{}
    \begin{center}
      Treinar um agente capaz de vencer algum jogo de Atari 2600
    \end{center}
  \end{block}
}

\subsection{Q-Learning}

\frame {
  \frametitle{Q-Learning}
  
  Algoritmo de aprendizagem por reforço. Dados um agente, um conjunto
  de estados $S$, um conjunto de ações $A$, temos:

  \begin{itemize}
    \item A partir de um estado $s \in S$, agente executa uma ação $a
      \in A$.
    \item Uma recompensa (valor numérico) é dada ao agente após cada
      ação.
  \end{itemize}
  
}

\frame {
  \frametitle{Q-Learning}
  
  Definimos uma função $Q : S \times A \rightarrow R$. O algoritmo baseia-se em
  iteração de valores:

  \begin{block}{}
    \begin{equation*}
      Q(s_t, a_t) \leftarrow (1 - \alpha)Q(s_t, a_t) + \alpha(r_t + \gamma \max_{a'}Q(s_{t+1}, a'))
    \end{equation*}
  \end{block}
  
  Onde $\alpha$ é a taxa de aprendizagem e $\gamma$ é o fator de
  desconto (determina o quão importante são as recompensas futuras).
}

\frame {
  \frametitle{Q-Learning}

  Após convergência, a função ótima $Q^{*}(s, a)$ é encontrada.
}

\subsection{Problema}

\frame {
  \frametitle{Problema}

  Função $Q^{*}(s,a)$ é estimada para cada par (estado, ação). Se o
  total de ações e estados for suficientemente grande, ou se o domínio
  do conjunto de estados contiver valores reais, tal abordagem
  torna-se impraticável.
  
}


\subsection{Solução}

\frame {
  \frametitle{Solução}

  Utilizar uma função de aproximação:

  \begin{block}{}
    \begin{equation*}
      Q(s,a,\theta) \approx Q^{*}(s,a)
    \end{equation*}
  \end{block}
}

\frame {
  \frametitle{Solução}

  Usar redes neurais como função de aproximação é uma boa idéia?

}

\section{Redes Neurais}

\subsection{Dificuldade}

\frame {
  \frametitle{Dificuldades}

  Segundo \cite{baird}, \emph{Q-Learning} pode não convergir de
  maneira apropriada quando implementado sobre sistemas generalizados
  de aproximações de funções, como é o caso de redes neurais com
  ativação não-linear.
    
  \begin{itemize}
    \item Sucessão de sequências não são independentes.
    \item Pequenas alterações nos Q-valores podem fazer a política
      oscilar muito.
  \end{itemize}

}

\subsection{Propostas de melhorias}

\frame {
  \frametitle{Proposta de melhoria}

  \begin{itemize}
    \item Em \cite{dqn}, foi proposto um modelo de memória de repetição de
      experiências que, em conjunto com aprendizagem em lote numa rede
      neural convolucional, alcançou o estado da arte em 7 jogos de Atari
      2600.
    \item Falar de \emph{freeze target}
  \end{itemize}

}

\subsection{Repetição de experiência}

\frame{
  \frametitle{Repetição de experiência}
  
  \begin{itemize}
  \item Experiência do agente: tupla $(s_t, a_t, r_t, s_{t+1})$.
  \item Armazenar as tuplas em uma base $D$, chamada memória para repetições.
  \item Durante o treinamento, amostras de $D$ são utilizadas para
    construção do lote utilizado na atualização dos pesos.
  \end{itemize}
}

\frame{
  \frametitle{Vantagens}

  \begin{itemize}
  \item Uso mais eficiente dos dados.
  \item Previne aprendizagem com dados fortemente correlacionados.
  \item Suaviza o aprendizado ao agregar dados de várias tuplas
    anteriormente observadas, tornando a distribuição dos
    comportamentos de entrada menos enviesados.
  \end{itemize}
}

\section{Solução implementada}

\subsection{Descrição do agente}

\frame{
  \frametitle{Descrição do agente}

  Foi utilizada a biblioteca OpenAI-gym. Ela contém ferramentas para
  implementação e comparação de vários problemas de aprendizagem por
  reforço.

}

\frame{
  \frametitle{Descrição do agente}

  Especificamente para Atari 2600:

  \begin{itemize}
  \item Encapsula o emulador Stella.
  \item Para cada jogo, é responsável pela entrada e saída de dados,
    além da geração das recompensas.
  \item Controle de \emph{Frame-skip}: cada ação é executada por 3, 4
    ou 5 \emph{frames}.
  \end{itemize}
}

\subsection{Implementação}

\frame{
  \frametitle{Estrutura da solução}
  
  \begin{enumerate}
  \item Dados de entrada: RAM do Atari 2600 (trivia: qual o tamanho?).
  \item Rede neural com ativação não-linear como aproximação para
    $Q^{*}(s,a)$.
  \item Atualizações em lote com repetição de experiência.
  \end{enumerate}
}

\frame{ 
  \frametitle{Estrutura da rede neural}

  \begin{enumerate}
  \item \emph{inputLayer = RAM(128)}
  \item \emph{hiddenLayer1 = DenseLayer(RAM, 1024, ReLU)}
  \item \emph{batchNorm1 = BatchNormalization(hiddenLayer1)}
  \item \emph{hiddenLayer2 = DenseLayer(batchNorm1, 1024, ReLU)}
  \item \emph{batchNorm2 = BatchNormalization(hiddenLayer2)}
  \item \emph{outputLayer = DenseLayer(batchNorm2, numberOfActions, no activation)}
  \end{enumerate}

}

\frame{
  \frametitle{Normalização do lote}

  Em aprendizagem de máquina, é usual aplicar mudança de escala dos
  dados de entrada: intervalo limitado ou normalização.

  A mesma idéia pode ser utilizada para cada camada da rede neural.
  
}

\frame{
  \frametitle{Normalização do lote}

  No treinamento, dados valores $x$ de um lote $B = \{x_1, \dots, m\}$, efetuamos:

  \begin{enumerate}
  \item $\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_i$
  \item $\sigma^2_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2$
  \item $\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$
  \item $y_i = \gamma \hat{x_i} + \beta$
  \end{enumerate}

  Durante o teste, podem ser utilizados os valores da média e
  variância amostral de todas as entradas observadas até o momento
  pela rede neural e seguir a partir do passo 3.

}

\frame{
  \frametitle{Resultado}

  Mostrar vídeo do modelo treinado.
}

\begin{frame}
  \frametitle{References}
  \footnotesize{
    \begin{thebibliography}{99}
      
    \bibitem[Sygnowski et al., 2016]{atari_ram} Jakub Sygnowsk and Henryk Michaelewski (2016)
      \newblock Learning from the Memory of Atari 2600
      
    \bibitem[Mnih et al., 2103]{dqn} Volodymyr Mnih and
      Koray Kavukcuoglu and
      David Silver and
      Alex Graves and
      Ioannis Antonoglou and
      Daan Wierstra and
      Martin A. Riedmiller (2013)
      \newblock Learning from the Memory of Atari 2600
      
    \bibitem[Ioff et al., 2015]{batch_norm} Sergey Ioffe and Christian Szegedy (2015)
      \newblock Batch Normalization: Accelerating Deep Network Training by Reducing
      Internal Covariate Shift

    \bibitem[Baird, 1995]{baird} Leemon Baird (1995)
      \newblock Residual algorithms: Reinforcement learning with function approximation    
            
    \end{thebibliography}
        
  }
\end{frame}


\begin{frame}[t, allowframebreaks]
\frametitle{References}
\bibliographystyle{amsalpha}
\bibliography{bibfile}
\end{frame}

\frame{
  \frametitle{Repositório}
  \begin{center}
    https://github.com/ricardoy/atari\_rl
  \end{center}
}

\frame {
  \frametitle{Fim}
      
  \begin{center}
    Obrigado!
  \end{center}

}

\end{document}
