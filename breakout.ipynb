{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python27.zip',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/plat-linux2',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/lib-tk',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/lib-old',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/lib-dynload',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/site-packages',\n",
       " '/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/site-packages/IPython/extensions',\n",
       " '/home/ricardo/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ricardo/anaconda2_5.1/envs/rl/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import sleep\n",
    "from numpy.random import randint\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "from keras.initializers import normal, identity\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD , Adam, Adagrad, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Dense, Merge\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'Breakout-ram-v0'\n",
    "aux = gym.make(GAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be configured\n",
    "BATCH_SIZE = 10\n",
    "MAX_ITERATIONS_PER_EPISODE = 200\n",
    "LEARNING_RATE = 0.00025\n",
    "TARGET_UPDATE_LIMIT = 1\n",
    "EPSILON = 1.0\n",
    "EPSILON_UPDATE = 0.1\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.95\n",
    "TERMINAL_REWARD = 0\n",
    "NUMBER_OF_FRAMES = 1\n",
    "\n",
    "# cannot be configured\n",
    "INPUT_SIZE = aux.observation_space.shape[0]\n",
    "OUTPUT_SIZE = aux.action_space.n\n",
    "MIN_EXPERIENCE_REPLAY_SIZE = 1000\n",
    "MAX_EXPERIENCE_REPLAY_SIZE = 100000\n",
    "DATA_TYPE = np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate(dqn.last_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dqn.replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random positions for average q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random states already exists\n"
     ]
    }
   ],
   "source": [
    "SIZE=10000\n",
    "\n",
    "random_states_filename = 'random_states_breakout.h5'\n",
    "if not os.path.isfile(random_states_filename):\n",
    "    print 'Generating random states'\n",
    "    with h5py.File(random_states_filename, 'w') as h5:\n",
    "        random_states = h5.create_dataset('random_states', (SIZE, NUMBER_OF_FRAMES, INPUT_SIZE), dtype=DATA_TYPE)\n",
    "        data = deque()\n",
    "        data.append(aux.reset())\n",
    "        done = False\n",
    "        for i in range(0, SIZE):\n",
    "            if done:\n",
    "                data.clear()\n",
    "                state = aux.reset()\n",
    "                done = False\n",
    "            else:\n",
    "                state, _, done, _ = aux.step(aux.action_space.sample())                \n",
    "            \n",
    "            data.append(state)\n",
    "            if (len(data) > NUMBER_OF_FRAMES):\n",
    "                data.popleft()\n",
    "            if (len(data) == NUMBER_OF_FRAMES):\n",
    "                random_states[i] = state\n",
    "else:\n",
    "    print 'Random states already exists'\n",
    "            \n",
    "random_states = np.zeros((SIZE, NUMBER_OF_FRAMES * INPUT_SIZE), dtype=DATA_TYPE)\n",
    "with h5py.File(random_states_filename, 'r') as h5:\n",
    "    X = h5.get('random_states')\n",
    "    for i in range(0, SIZE):\n",
    "        for j in range(0, NUMBER_OF_FRAMES):\n",
    "            random_states[i] = np.concatenate(X[i])\n",
    "            \n",
    "random_states = random_states / 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate):\n",
    "    \"\"\"\"Return the neural network\"\"\"        \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, kernel_initializer='he_normal', activation='relu', input_dim=(INPUT_SIZE * NUMBER_OF_FRAMES)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1024, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(OUTPUT_SIZE, kernel_initializer='truncated_normal'))\n",
    "\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "class DeepQNetwork:\n",
    "    \n",
    "    def __init__(self, **kwargs):       \n",
    "        if 'batch_size' in kwargs:\n",
    "            self.batch_size = kwargs['batch_size']\n",
    "        else:\n",
    "            self.batch_size = BATCH_SIZE\n",
    "\n",
    "        if 'min_experience_replay_size' in kwargs:\n",
    "            self.minimum_experience_replay_size = kwargs['min_experience_replay_size']\n",
    "        else:\n",
    "            self.minimum_experience_replay_size = MIN_EXPERIENCE_REPLAY_SIZE\n",
    "            \n",
    "        if 'learning_rate' in kwargs:\n",
    "            self.learning_rate = kwargs['learning_rate']\n",
    "        else:\n",
    "            self.learning_rate = LEARNING_RATE\n",
    "\n",
    "        if 'epsilon' in kwargs:\n",
    "            self.epsilon = kwargs['epsilon']\n",
    "        else:\n",
    "            self.epsilon = EPSILON\n",
    "        \n",
    "        if 'gamma' in kwargs:\n",
    "            self.gamma = kwargs['gamma']\n",
    "        else:\n",
    "            self.gamma = GAMMA\n",
    "            \n",
    "        if 'target_update_limit' in kwargs:\n",
    "            self.target_update_limit = kwargs['target_update_limit']\n",
    "        else:\n",
    "            self.target_update_limit = TARGET_UPDATE_LIMIT\n",
    "            \n",
    "        if 'max_iterations_per_episode' in kwargs:\n",
    "            self.max_iterations_per_episode = kwargs['max_iterations_per_episode']\n",
    "        else:\n",
    "            self.max_iterations_per_episode = MAX_ITERATIONS_PER_EPISODE\n",
    "            \n",
    "        if 'min_epsilon' in kwargs:\n",
    "            self.min_epsilon = kwargs['min_epsilon']\n",
    "        else:\n",
    "            self.min_epsilon = MIN_EPSILON\n",
    "            \n",
    "        if 'terminal_reward' in kwargs:\n",
    "            self.terminal_reward = kwargs['terminal_reward']\n",
    "        else:\n",
    "            self.terminal_reward = TERMINAL_REWARD\n",
    "            \n",
    "        if 'epsilon_update' in kwargs:\n",
    "            self.epsilon_update = kwargs['epsilon_update']\n",
    "        else:\n",
    "            self.epsilon_update = EPSILON_UPDATE\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.last_states = deque()\n",
    "        self.total_last_states = 0\n",
    "        \n",
    "        self.replay = deque()\n",
    "        self.total_replay = 0\n",
    "        \n",
    "        self.env = gym.make(GAME)\n",
    "        \n",
    "        self.reward_sum = 0\n",
    "        self.total_episodes = 0\n",
    "        \n",
    "        self.episode_iterations = 0\n",
    "        self.target_update = 0                \n",
    "        \n",
    "        self.model = build_model(self.learning_rate)\n",
    "        self.frozen_model = build_model(self.learning_rate)\n",
    "        \n",
    "        \n",
    "    def choose_best_action(self):\n",
    "        \"\"\"Return the action a that maximizes q(self.last_states, a)\"\"\"\n",
    "        s = np.array([np.concatenate([self.last_states]).reshape(NUMBER_OF_FRAMES * INPUT_SIZE)])\n",
    "        q = self.model.predict(s/256.)[0]\n",
    "        action = np.argmax(q)\n",
    "        return action, q[action]\n",
    "\n",
    "    \n",
    "    def choose_random_action(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    \n",
    "    def choose_e_greedy_action(self):\n",
    "        \"\"\"Return an action chosen following the e-greedy policy\"\"\"\n",
    "        if random.random() <= self.epsilon or self.total_last_states < NUMBER_OF_FRAMES:\n",
    "            return self.choose_random_action()\n",
    "        else:\n",
    "            action, _ = self.choose_best_action()\n",
    "            return action\n",
    "        \n",
    "        \n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Return the reward for executing the action and a boolean\n",
    "           indicating if the new state is terminal.\n",
    "        \n",
    "        \"\"\"\n",
    "        state, original_reward, done, _ = self.env.step(action)        \n",
    "        reward = original_reward\n",
    "        self.reward_sum += reward\n",
    "        \n",
    "#         if done:\n",
    "#             reward = self.terminal_reward\n",
    "        \n",
    "        self.add_replay(action, reward, state, done)        \n",
    "        \n",
    "        self.episode_iterations += 1\n",
    "        if done or self.episode_iterations >= self.max_iterations_per_episode:\n",
    "            self.episode_iterations = 0\n",
    "            done = True\n",
    "            self.total_episodes += 1\n",
    "            self.reset_states()\n",
    "        else:\n",
    "            self.add_state(state)\n",
    "                \n",
    "        return original_reward, done\n",
    "    \n",
    "    \n",
    "    def add_replay(self, action, reward, state, done):\n",
    "        previous_state = None\n",
    "        if self.total_last_states == NUMBER_OF_FRAMES:\n",
    "            previous_state = self.copy_last_states()\n",
    "            \n",
    "        self.add_state(state)\n",
    "            \n",
    "        if previous_state is not None:\n",
    "            current_state = self.copy_last_states()            \n",
    "            self.replay.append((previous_state, action, reward, current_state, done))\n",
    "            self.total_replay += 1\n",
    "            if self.total_replay > MAX_EXPERIENCE_REPLAY_SIZE:\n",
    "                self.replay.popleft()      \n",
    "                self.total_replay -= 1\n",
    "\n",
    "    \n",
    "    def copy_last_states(self):\n",
    "        v = []\n",
    "        for s in self.last_states:\n",
    "            v.append(np.copy(s))\n",
    "        return v\n",
    "    \n",
    "    \n",
    "    def add_state(self, state):\n",
    "        self.last_states.append(np.array(state, dtype=DATA_TYPE))\n",
    "        self.total_last_states += 1\n",
    "        if self.total_last_states > NUMBER_OF_FRAMES:\n",
    "            self.last_states.popleft()\n",
    "            self.total_last_states -= 1\n",
    "            \n",
    "            \n",
    "    def reset_states(self):\n",
    "        self.last_states.clear()\n",
    "        self.last_states.append(np.array(self.env.reset(), dtype=DATA_TYPE))\n",
    "        self.total_last_states = 1            \n",
    "        \n",
    "    def run_test_average_reward(self, total_episodes, render): \n",
    "        \"\"\"Run the environment without traning.\n",
    "        \n",
    "           Keyword arguments:\n",
    "           total_episodes -- number of times the environment \n",
    "               will be run.\n",
    "           render -- boolean indicating if the screen must be\n",
    "               rendered\n",
    "        \"\"\"\n",
    "        reward_sum = 0\n",
    "        total_iterations = 0\n",
    "                \n",
    "        for _ in range(0, total_episodes):\n",
    "            self.episode_iterations = 0\n",
    "            self.reset_states()\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    sleep(0.03)\n",
    "                    self.env.render()\n",
    "                \n",
    "                if self.total_last_states < NUMBER_OF_FRAMES:\n",
    "                    action = self.choose_random_action()\n",
    "                else:  \n",
    "#                     print self.total_last_states\n",
    "#                     print self.last_states.shape\n",
    "                    action, _ = self.choose_best_action()\n",
    "                total_iterations += 1\n",
    "                \n",
    "                r, done = self.execute_action(action)\n",
    "                reward += r              \n",
    "            reward_sum += reward            \n",
    "            \n",
    "        avg_reward = reward_sum / float(total_iterations)\n",
    "        \n",
    "        return avg_reward\n",
    "    \n",
    "    \n",
    "    def run_test_average_qvalue(self):\n",
    "        \"\"\"Calculate the average max q-value for the random_states\"\"\"\n",
    "        y = self.model.predict(random_states)        \n",
    "        return np.average(np.amax(y, axis=1))\n",
    "        \n",
    "    def update_network(self):\n",
    "        \"\"\"Execute a mini-batch update\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.replay, self.batch_size - 1)\n",
    "        batch.append(self.replay[-1])\n",
    "\n",
    "        X_last = np.zeros((self.batch_size, NUMBER_OF_FRAMES * INPUT_SIZE), dtype=DATA_TYPE)\n",
    "        X_current = np.zeros((self.batch_size, NUMBER_OF_FRAMES * INPUT_SIZE), dtype=DATA_TYPE)        \n",
    "\n",
    "        for i in range(0, self.batch_size):\n",
    "            ls, la, r, s, d = batch[i]\n",
    "            X_last[i] = np.concatenate([ls]).reshape(NUMBER_OF_FRAMES * INPUT_SIZE)\n",
    "            X_current[i] = np.concatenate([s]).reshape(NUMBER_OF_FRAMES * INPUT_SIZE)\n",
    "            \n",
    "        y = self.model.predict(X_last/256.)\n",
    "        \n",
    "        self.target_update += 1        \n",
    "        if self.target_update >= self.target_update_limit:\n",
    "            self.target_update = 0\n",
    "            self.frozen_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "        q_theta = self.frozen_model.predict(X_current/256.)\n",
    "        \n",
    "        for i in range(0, self.batch_size):\n",
    "            _, la, r, _, d = batch[i]\n",
    "\n",
    "            if d:\n",
    "                score = r\n",
    "            else:\n",
    "                score = r + self.gamma * np.max(q_theta[i])\n",
    "\n",
    "            y[i][la] = score\n",
    "\n",
    "        loss = self.model.train_on_batch(X_last/256., y)\n",
    "#         loss = self.model.fit(X_last, y, batch_size=32, nb_epoch=3, verbose=1)        \n",
    "            \n",
    "    def train(self, total_frames, render):\n",
    "        \"\"\"Run the neural network training\n",
    "        \n",
    "           Keyword arguments:\n",
    "           total_frames -- number of times the training process will be executed\n",
    "           render -- if the screen should be rendered\n",
    "           \n",
    "        \"\"\"\n",
    "        self.reset_states()\n",
    "               \n",
    "        training_iterations = 0\n",
    "        \n",
    "        self.episode_iterations = 0\n",
    "        while(training_iterations < total_frames):\n",
    "            if render:\n",
    "                self.env.render()           \n",
    "        \n",
    "            action = self.choose_e_greedy_action()\n",
    "            _, done = self.execute_action(action)\n",
    "            \n",
    "            if self.total_replay > MIN_EXPERIENCE_REPLAY_SIZE:\n",
    "                training_iterations += 1\n",
    "                self.update_network()\n",
    "                \n",
    "            if done:\n",
    "                self.reset_states()\n",
    "                \n",
    "        # update epsilon        \n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_update)\n",
    "\n",
    "                \n",
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.best_score = None\n",
    "        \n",
    "    def add_data(self, epoch, average_reward, average_qvalue):\n",
    "        t = (epoch, average_reward, average_qvalue)\n",
    "        print 'epoch: {} avg_reward: {} avg_qvalue: {}'.format(epoch, average_reward, average_qvalue)\n",
    "        self.data.append(t)\n",
    "        \n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        df = pd.DataFrame(self.data, columns=['epoch', 'avg_reward', 'avg_qvalue'])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def execute(self, epochs, dqn, model_name):    \n",
    "        self.best_score = None \n",
    "        self.best_qvalue = None\n",
    "        self.best_filename = None\n",
    "        c = 0\n",
    "        \n",
    "        for i in tqdm(range(epochs)):\n",
    "            c += 1\n",
    "            \n",
    "            filename = '/tmp/%s.%03d.h5'%(model_name, c)\n",
    "            \n",
    "            # train\n",
    "            dqn.train(2000, False)                 \n",
    "\n",
    "            # test\n",
    "            avg_reward = dqn.run_test_average_reward(100, False)            \n",
    "            avg_qvalue = dqn.run_test_average_qvalue()\n",
    "            if self.best_score is None or avg_reward > self.best_score or \\\n",
    "                (avg_reward == self.best_score and avg_qvalue >= self.best_qvalue):\n",
    "                    \n",
    "                self.best_score = avg_reward\n",
    "                self.best_qvalue = avg_qvalue\n",
    "                self.best_filename = filename\n",
    "\n",
    "            self.add_data(c, avg_reward, avg_qvalue)\n",
    "\n",
    "            # save model            \n",
    "            dqn.model.save_weights(filename)\n",
    "\n",
    "            \n",
    "def generate_graphic(filename, adjusted_qvalue, qvalue_range=[-100,150], reward_range=[0,250]):\n",
    "    sns.set_context(\"paper\")\n",
    "    sns.set_style('dark')\n",
    "    \n",
    "    df = pd.DataFrame(experiment.get_dataframe(), columns=['epoch', 'avg_reward', 'avg_qvalue'])\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(df['epoch'], df['avg_reward'])\n",
    "    if reward_range is not None:\n",
    "        ax1.set_ylim(reward_range)\n",
    "    ax1.set_xlabel('Number of epochs')\n",
    "    ax1.set_ylabel('Average reward')\n",
    "    \n",
    "    ax1.get_xaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    ax1.get_yaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n",
    "    ax1.grid(b=True, which='major', color='w', linewidth=1.5)\n",
    "    ax1.grid(b=True, which='minor', color='w', linewidth=0.5)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['epoch'], df['avg_qvalue'] + adjusted_qvalue, color='r')\n",
    "    ax2.set_ylabel('Average q-value')\n",
    "    if qvalue_range is not None:\n",
    "        ax2.set_ylim(qvalue_range)\n",
    "    \n",
    "    ax1.legend(loc=2)\n",
    "    ax2.legend(loc=0)\n",
    "    \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "The running timing for most of training is about 4 to 10 minutes, except when the maximum number of iterations in an episode is greater than the default value (200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model - restricted\n",
    "- target update 200\n",
    "- gamma 0.99\n",
    "- reward at end -100\n",
    "- learning rate 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch, average_reward, average_qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/500 [02:48<23:18:40, 168.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 avg_reward: 0.0 avg_qvalue: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/500 [05:35<23:13:58, 167.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 avg_reward: 0.0 avg_qvalue: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 3/500 [09:00<24:52:53, 180.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 avg_reward: 0.0 avg_qvalue: nan\n"
     ]
    }
   ],
   "source": [
    "label = 'best_restricted'\n",
    "\n",
    "config = {\n",
    "    'epsilon': 0.5,\n",
    "    'gamma' : 0.9,\n",
    "    'target_update_limit' : 100,\n",
    "    'learning_rate' : 0.0001,\n",
    "    'max_iterations_per_episode' : 1000\n",
    "}\n",
    "\n",
    "# dqn = DeepQNetwork(**config)\n",
    "dqn.batch_size = 50\n",
    "\n",
    "experiment = Experiment()\n",
    "experiment.execute(500, dqn, label)\n",
    "print 'Best iteration'\n",
    "print 'score: {} | qvalue: {} | model: {}' \\\n",
    "    .format(experiment.best_score, experiment.best_qvalue, experiment.best_filename)\n",
    "generate_graphic('report/images/{}.pdf'.format(label), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model - unrestricted\n",
    "- target update 200\n",
    "- gamma 0.99\n",
    "- reward at end -100\n",
    "- max iterations per episode 1000\n",
    "- learning rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = 'best_unrestricted'\n",
    "\n",
    "config = {\n",
    "    'terminal_reward' : -100,\n",
    "    'gamma' : 0.9,\n",
    "    'target_update_limit' : 200,\n",
    "    'max_iterations_per_episode' : 1000,\n",
    "    'learning_rate' : 0.001\n",
    "}\n",
    "\n",
    "# dqn = DeepQNetwork(**config)\n",
    "\n",
    "experiment = Experiment()\n",
    "experiment.execute(500, dqn, label)\n",
    "print 'Best iteration'\n",
    "print 'score: {} | qvalue: {} | model: {}' \\\n",
    "    .format(experiment.best_score, experiment.best_qvalue, experiment.best_filename)\n",
    "generate_graphic('report/images/{}.pdf'.format(label), 0, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the graphic for previous model, setting the y_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render an episode using best model found - iteration 090 from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.00589721988206\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'max_iterations_per_episode' : 5000000\n",
    "}\n",
    "\n",
    "dqn_test = DeepQNetwork(**config)\n",
    "dqn_test.model.load_weights('boxing.h5')\n",
    "score = dqn_test.run_test_average_reward(1, False)\n",
    "print 'Score: {}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = dqn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-a1341ae929bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test_average_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-bd4d4c40a10d>\u001b[0m in \u001b[0;36mrun_test_average_reward\u001b[0;34m(self, total_episodes, render)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn.run_test_average_reward(1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
